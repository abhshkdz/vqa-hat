<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  <meta name="author" content="Abhishek Das">
  <meta name="keywords" content="yoda, abhshkdz, abhishek das, sdslabs, jekyll, hacker, developer, pianist, iit roorkee, abdasuee, deep learning, computer vision, neural networks, virginia tech, machine learning">
  <link rel="me" href="//plus.google.com/u/0/100610196494221761914/">
  <link rel="me" href="//facebook.com/abhshkdz">


    <link rel="shortcut icon" href="img/ico/favicon-96x96.png">


  <title>Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?</title>
  <meta name="description" content="We conduct large-scale studies on 'human attention' in Visual Question Answering (VQA) to understand where humans choose to look to answer questions about images. We design and test multiple game-inspired novel attention-annotation interfaces that require the subject to sharpen regions of a blurred image to answer a question. Thus, we introduce the VQA-HAT (Human ATtention) dataset. We evaluate attention maps generated by state-of-the-art VQA models against human attention both qualitatively (via visualizations) and quantitatively (via rank-order correlation). Overall, our experiments show that current attention models in VQA do not seem to be looking at the same regions as humans.">

  <!-- CSS  -->
  <link rel="stylesheet" type="text/css" href="css/bootstrap.min.css">
  <link rel="stylesheet" type="text/css" href="css/main.css" media="screen,projection">
</head>
  <body>

    <div class="top-strip"></div>
    <div class="container">
      <div class="page-header" id="banner">
  <div class="row">
    <div class="col-xs-12">
      <h1>
        Human Attention in Visual Question Answering:<br>Do Humans and Deep Networks Look at the Same Regions?


      </h1>
      <p class="lead authors"><a href="http://abhishekdas.com">Abhishek Das</a>*, <a href="//www.linkedin.com/in/harsh092">Harsh Agrawal</a>*, <a href="http://larryzitnick.org">C. Lawrence Zitnick</a>, <a href="//computing.ece.vt.edu/~parikh/">Devi Parikh</a>, <a href="//computing.ece.vt.edu/~dbatra/">Dhruv Batra</a></p>
    </div>
  </div>
</div>
      <div class="page-content">
          <div class="row">
    <div class="col-xs-12">
        <h2>Abstract</h2>
    </div>
</div>
<div class="row">
    <div class="col-xs-12">
        <p>We conduct large-scale studies on 'human attention' in Visual Question Answering (VQA) to understand where humans choose to look to answer questions about images. We design and test multiple game-inspired novel attention-annotation interfaces that require the subject to sharpen regions of a blurred image to answer a question. Thus, we introduce the VQA-HAT (Human ATtention) dataset. We evaluate attention maps generated by state-of-the-art VQA models against human attention both qualitatively (via visualizations) and quantitatively (via rank-order correlation). Overall, our experiments show that current attention models in VQA do not seem to be looking at the same regions as humans.</p>
        <div>Read more in the <a href="http://arxiv.org/abs/1606.03556">paper</a>.</div>
        <a href="http://arxiv.org/abs/1606.03556"><img class="thumb" src="img/thumb.jpg" /></a>
        <div>To appear in <a href="http://www.emnlp2016.net/">EMNLP 2016</a>. Initial version presented at <a href="//icmlviz.github.io/">ICML 2016 Workshop on Visualization for Deep Learning</a> (Best student paper).</div>
    </div>
</div>
<hr />

<div class="row">
  <div class="col-xs-12">
    <h2>Bibtex</h2>

<pre>@inproceedings{vqahat,
  title={{Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?}},
  author={Abhishek Das and Harsh Agrawal and C. Lawrence Zitnick and Devi Parikh and Dhruv Batra},
  booktitle={Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2016}
}</pre>

  </div>
</div>
<hr />

<div class="row">
  <div class="col-xs-12">
    <h2>VQA-HAT Dataset</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      Human attention image files are named as "<code>qid</code>_<code>n</code>.png".
      <code>qid</code> refers to question ID according to the <a href="http://visualqa.org/">VQA dataset (v1.0)</a>, and <code>n</code> iterates over multiple attention maps per question.
      For example, 1500070_1.png, 1500070_2.png, etc.
      n = 1 for the training set and n = {1,2,3} for the validation set.
      Download links are given below.
    </p>
  </div>
</div>
<div class="row">
  <div class="col-sm-4">
    <a href="http://s3.amazonaws.com/vqa-hat/vqahat_train.zip">Training set</a> (703M)<br />58,475 attention maps
  </div>
  <div class="col-sm-4">
    <a href="http://s3.amazonaws.com/vqa-hat/vqahat_val.zip">Validation set</a> (47M)<br />4,122 attention maps
  </div>
</div>
<hr />

<div class="row">
  <div class="col-xs-12">
    <h2>Human vs. Machine Attention</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      Machine-generated attention maps on COCO-val for Stacked Attention Network
      (Yang et al., CVPR16), HieCoAtt (Lu et al., NIPS16) and Judd et al, ICCV09
      are available for download <a href="/data/vqahat/">here</a>.
      For SAN and HieCoAtt, image files are named as "<code>qid</code>.png", and
      for Judd et al., "<code>coco_image_id</code>.jpg".
      Examples of human attention (column 2) vs. machine-generated attention
      (columns 3-5) with rank-correlation coefficients are shown below.
    </p>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <img src="img/att_comparison.jpg" />
  </div>
</div>
<hr />


      </div>

<div class="row">
  <div class="col-xs-12">
    <h2>Acknowledgements</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      We thank Jiasen Lu and Ramakrishna Vedantam for helpful suggestions and discussions. This work was supported in part by the following: National Science Foundation CAREER awards to DB and DP, Army Research Office YIP awards to DB and DP, ICTAS Junior Faculty awards at Virginia Tech to DB and DP, Army Research Lab grant W911NF-15-2-0080 to DP and DB, Office of Naval Research YIP award to DP, Office of Naval Research grant N00014-14-1-0679 to DB, Alfred P. Sloan Fellowship to DP, Paul G. Allen Family Foundation Allen Distinguished Investigator award to DP, Google Faculty Research award to DP and DB, AWS in Education Research grant to DB, and NVIDIA GPU donation to DB.
    </p>
  </div>
</div>

    </div>

    <!-- Google analytics -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-79939700-1', 'auto');
      ga('send', 'pageview');
    </script>

  </body>
</html>